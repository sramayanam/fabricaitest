{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000001",
   "metadata": {},
   "source": [
    "# PostgreSQL → OneLake Pipeline Template\n",
    "\n",
    "This Microsoft Fabric Spark notebook:\n",
    "1. Connects to a **PostgreSQL** source database.\n",
    "2. Introspects the schema of the requested table from `information_schema.columns`.\n",
    "3. Maps every PostgreSQL column data type to its OneLake / Spark equivalent using `type_mappings.py`.\n",
    "4. Reads the full table with the mapped schema.\n",
    "5. Writes the result as a **Delta table** into the target OneLake Lakehouse.\n",
    "\n",
    "## Prerequisites\n",
    "* A PostgreSQL JDBC driver JAR available to the Spark cluster  \n",
    "  *(add it as a library attachment in the Fabric Spark environment)*.\n",
    "* `type_mappings.py` uploaded to the Lakehouse `Files/` root or co-located with this notebook.\n",
    "* A Fabric **Secret** (Key Vault) or Notebook parameter for the database password.\n",
    "\n",
    "## Parameters\n",
    "Use the **Parameterize** toggle in Fabric to set these values at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000002",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Pipeline parameters — override these at runtime via Fabric pipeline\n",
    "# or the 'Parameterize' feature of a Fabric notebook.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Source PostgreSQL connection\n",
    "PG_HOST: str = \"localhost\"          # e.g. \"my-pg-server.postgres.database.azure.com\"\n",
    "PG_PORT: int = 5432\n",
    "PG_DATABASE: str = \"source_db\"      # database name to connect to\n",
    "PG_USER: str = \"postgres\"\n",
    "PG_PASSWORD: str = \"\"               # set via Key Vault / secret — do NOT hard-code\n",
    "PG_SCHEMA: str = \"public\"           # PostgreSQL schema (namespace)\n",
    "PG_TABLE: str = \"\"                  # table name to migrate\n",
    "\n",
    "# Target OneLake / Lakehouse\n",
    "LAKEHOUSE_TABLE: str = \"\"           # target Delta table name in the Lakehouse\n",
    "                                    # defaults to PG_TABLE when left blank\n",
    "WRITE_MODE: str = \"overwrite\"       # \"overwrite\" | \"append\" | \"merge\"\n",
    "DELTA_PARTITION_COLS: list = []     # optional list of column names to partition by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000003",
   "metadata": {},
   "source": [
    "## 1 — Imports & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Add the Lakehouse Files root to sys.path so type_mappings can be imported.\n",
    "# Adjust the path if type_mappings.py lives elsewhere.\n",
    "sys.path.insert(0, \"/lakehouse/default/Files\")\n",
    "\n",
    "from type_mappings import build_spark_schema, map_pgsql_type_to_spark  # noqa: E402\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000005",
   "metadata": {},
   "source": [
    "## 2 — Build JDBC Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url: str = (\n",
    "    f\"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "    \"?sslmode=require\"  # recommended for Azure Database for PostgreSQL\n",
    ")\n",
    "\n",
    "jdbc_properties: dict = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "\n",
    "print(f\"JDBC URL: jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}?sslmode=require\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000007",
   "metadata": {},
   "source": [
    "## 3 — Fetch Column Schema from PostgreSQL `information_schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query information_schema.columns to discover column names and data types.\n",
    "# We also pull numeric_precision / numeric_scale for DECIMAL columns and\n",
    "# is_nullable so we can faithfully replicate the schema.\n",
    "schema_query: str = f\"\"\"\n",
    "    SELECT\n",
    "        column_name,\n",
    "        data_type,\n",
    "        numeric_precision,\n",
    "        numeric_scale,\n",
    "        is_nullable,\n",
    "        ordinal_position\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = '{PG_SCHEMA}'\n",
    "      AND table_name   = '{PG_TABLE}'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\"\n",
    "\n",
    "schema_df = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"query\", schema_query)\n",
    "    .option(\"user\", PG_USER)\n",
    "    .option(\"password\", PG_PASSWORD)\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "schema_rows: list = schema_df.collect()\n",
    "\n",
    "if not schema_rows:\n",
    "    raise ValueError(\n",
    "        f\"No columns found for '{PG_SCHEMA}.{PG_TABLE}' in database '{PG_DATABASE}'. \"\n",
    "        \"Check that the table exists and the user has SELECT privileges on information_schema.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(schema_rows)} column(s) in {PG_SCHEMA}.{PG_TABLE}:\")\n",
    "schema_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000009",
   "metadata": {},
   "source": [
    "## 4 — Map PostgreSQL Types → Spark / OneLake Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each Row to a plain dict so build_spark_schema can consume it.\n",
    "column_descriptors: list[dict] = [\n",
    "    {\n",
    "        \"column_name\":       row[\"column_name\"],\n",
    "        \"data_type\":         row[\"data_type\"],\n",
    "        \"numeric_precision\": row[\"numeric_precision\"],\n",
    "        \"numeric_scale\":     row[\"numeric_scale\"],\n",
    "        \"is_nullable\":       row[\"is_nullable\"],\n",
    "    }\n",
    "    for row in schema_rows\n",
    "]\n",
    "\n",
    "spark_schema = build_spark_schema(column_descriptors)\n",
    "\n",
    "print(\"\\nMapped Spark schema:\")\n",
    "print(spark_schema.simpleString())\n",
    "\n",
    "# Pretty-print the per-column mapping for auditing / debugging\n",
    "print(\"\\n{:<35} {:<35} {}\".format(\"PostgreSQL type\", \"Spark type\", \"Column\"))\n",
    "print(\"-\" * 90)\n",
    "for col_desc, field in zip(column_descriptors, spark_schema.fields):\n",
    "    print(\"{:<35} {:<35} {}\".format(col_desc[\"data_type\"], str(field.dataType), field.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000011",
   "metadata": {},
   "source": [
    "## 5 — Read Source Table from PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualified_table: str = f'\"{PG_SCHEMA}\".\"{PG_TABLE}\"'\n",
    "\n",
    "source_df = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", qualified_table)\n",
    "    .option(\"user\", PG_USER)\n",
    "    .option(\"password\", PG_PASSWORD)\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    # Parallel read: tune numPartitions, partitionColumn, lowerBound,\n",
    "    # upperBound for large tables to improve throughput.\n",
    "    # .option(\"numPartitions\", \"8\")\n",
    "    # .option(\"partitionColumn\", \"id\")\n",
    "    # .option(\"lowerBound\", \"1\")\n",
    "    # .option(\"upperBound\", \"1000000\")\n",
    "    .schema(spark_schema)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(f\"Row count: {source_df.count():,}\")\n",
    "source_df.printSchema()\n",
    "source_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000013",
   "metadata": {},
   "source": [
    "## 6 — Write to OneLake Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table: str = LAKEHOUSE_TABLE if LAKEHOUSE_TABLE else PG_TABLE\n",
    "\n",
    "writer = source_df.write.format(\"delta\").mode(WRITE_MODE)\n",
    "\n",
    "if DELTA_PARTITION_COLS:\n",
    "    writer = writer.partitionBy(*DELTA_PARTITION_COLS)\n",
    "\n",
    "writer.saveAsTable(target_table)\n",
    "\n",
    "print(f\"✅  Table '{target_table}' written to OneLake in '{WRITE_MODE}' mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c1234-0000-4000-a000-000000000015",
   "metadata": {},
   "source": [
    "## 7 — (Optional) Verify Written Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c1234-0000-4000-a000-000000000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_df = spark.sql(f\"SELECT * FROM {target_table} LIMIT 10\")\n",
    "verification_df.show(truncate=True)\n",
    "print(f\"Total rows in OneLake table '{target_table}': {spark.table(target_table).count():,}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": false
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "",
    "default_lakehouse_name": "",
    "default_lakehouse_workspace_id": "",
    "known_lakehouses": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
